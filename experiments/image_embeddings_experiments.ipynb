{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nScript to generate embeddings for images (for catalog images) and store embeddings to Qdrant database.\\n@File    : image_embeddings_experiments.ipynb\\n@Author  : Nandini Bohra\\n@Contact : nbohra@ucsd.edu\\n\\n@References : https://www.youtube.com/watch?v=MlRkBvOCfTY\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to generate embeddings for images (for catalog images) and store embeddings to Qdrant database.\n",
    "@File    : image_embeddings_experiments.ipynb\n",
    "@Author  : Nandini Bohra\n",
    "@Contact : nbohra@ucsd.edu\n",
    "\n",
    "@References : https://www.youtube.com/watch?v=MlRkBvOCfTY\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# for image resizing to b64\n",
    "from io import BytesIO\n",
    "import math\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing in the payloads csv on all image information\n",
    "payloads = pd.read_csv(\"payloads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming local images into PIL images\n",
    "\n",
    "images = list(map(lambda x: Image.open(x), payloads[\"image_url\"]))\n",
    "images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing originals to smaller images and converting to base-64 rep if needed to show on front end\n",
    "\n",
    "target_width = 256\n",
    "\n",
    "# Resizing images to target width\n",
    "# Returns PIL image\n",
    "def resize_img(url):\n",
    "    pil_img = Image.open(url)\n",
    "    img_aspect_ratio = pil_img.width / pil_img.height\n",
    "    resized_img = pil_img.resize(\n",
    "        (target_width, math.floor(target_width * img_aspect_ratio))\n",
    "    )\n",
    "\n",
    "    return resized_img\n",
    "\n",
    "# Converting PIL image to base64 string\n",
    "def img_to_base64(pil_img):\n",
    "    image_data = BytesIO()\n",
    "    pil_img.save(image_data, format=\"JPEG\")\n",
    "    base64_string = base64.b64encode(image_data.getvalue()).decode(\"utf-8\")\n",
    "    return base64_string\n",
    "\n",
    "# Saving base64 reps to payloads dataframe\n",
    "resized_images = list(map(lambda x: resize_img(x), payloads[\"image_url\"])) \n",
    "base64_images = list(map(lambda x: img_to_base64(x), resized_images))\n",
    "payloads[\"base64\"] = base64_images\n",
    "payloads.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial with Microsoft Resnet-50 model\n",
    "# https://huggingface.co/microsoft/resnet-50\n",
    "\n",
    "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
    "model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
    "\n",
    "inputs = processor(\n",
    "    images, \n",
    "    return_tensors=\"pt\", \n",
    "    # padding=True\n",
    ")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "embeddings = outputs.logits\n",
    "embeddings\n",
    "\n",
    "# Evaluated embeddings... not sure if this is the right fit \n",
    "# Researching and trying other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying DINO V2 model \n",
    "# Less for object classification and more for fine details, textures --> may be suitable for textile catalog\n",
    "# https://huggingface.co/facebook/dinov2-base\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import torch\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "inputs = processor(\n",
    "    images, \n",
    "    return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "hidden_states = outputs.last_hidden_state # Shape: [batch_size=50, num_tokens=1+256, hidden_size=768]\n",
    "\n",
    "# Removing CLS Token and taking average of all patch embeddings\n",
    "# This is because CLS token is used for classification, semantics and not for image embeddings\n",
    "# Proceeding with average of all patch embeddings to retain more fine-grained details of image\n",
    "\n",
    "all_patch_embedding = hidden_states[:, 1:, :]\n",
    "print(all_patch_embedding.shape)\n",
    "\n",
    "avg_patch_embeddings = torch.mean(all_patch_embedding, dim=1)\n",
    "print(avg_patch_embeddings.shape)\n",
    "\n",
    "avg_patch_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINOv2 Model \n",
    "embedding_len = len(avg_patch_embeddings[0])\n",
    "embedding_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINOv2 Model \n",
    "# Visualizing cosine similarity matrix between embeddings...\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "embeddings = avg_patch_embeddings.detach().cpu().numpy()\n",
    "\n",
    "# calculate value to normalize each vector by\n",
    "norm_factor = np.linalg.norm(embeddings, axis=1)\n",
    "norm_factor.shape\n",
    "\n",
    "cos_sim = np.dot(embeddings, embeddings.T) / (\n",
    "    norm(embeddings, axis=1) * norm(embeddings, axis=1)\n",
    ")\n",
    "print(cos_sim.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(cos_sim)\n",
    "plt.title(\"Cosine Similarity Heatmap for DINOv2 Image Embeddings\")\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Image Index\")\n",
    "plt.ylabel(\"Image Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosing issues with image embeddings in DINOv2\n",
    "# Checking mean emb here to see if embs are 0-centered\n",
    "print(np.mean(embeddings, axis=0))\n",
    "\n",
    "# Checking histogram of similarities btw embeddings\n",
    "# Ideally want a good spread across the range \n",
    "# Any clustering n small ranges can be problematic\n",
    "cos_sim_values = cos_sim[np.triu_indices_from(cos_sim, k=1)]\n",
    "plt.hist(cos_sim_values, bins=50, alpha=0.75, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to zero-center embeddings to improve cosine similarity\n",
    "mean_embedding = np.mean(embeddings, axis=0)  \n",
    "centered_embeddings = embeddings - mean_embedding \n",
    "\n",
    "norm_embeddings = centered_embeddings / np.linalg.norm(centered_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "cos_sim = np.dot(norm_embeddings, norm_embeddings.T)\n",
    "\n",
    "cos_sim_values = cos_sim[np.triu_indices_from(cos_sim, k=1)]\n",
    "plt.hist(cos_sim_values, bins=50, alpha=0.75, color='blue')\n",
    "plt.show()\n",
    "\n",
    "# Not super helpful, just contrasts embeddings more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now working with CLIP instead to implement multimodal similarity search\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model = CLIPModel.from_pretrained(model_id)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "payloads['info'] = payloads['material'] + \" \" + payloads['color']\n",
    "tokens = processor(\n",
    "    text=payloads['info'].tolist(),\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ").to(device)\n",
    "tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Model \n",
    "# Getting text embeddings from CLIP\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_emb = model.get_text_features(**tokens)\n",
    "\n",
    "print(text_emb.shape)\n",
    "print(text_emb.min(), text_emb.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Model\n",
    "import numpy as np\n",
    "\n",
    "# detach text emb from graph, move to CPU, and convert to numpy array\n",
    "text_emb = text_emb.detach().cpu().numpy()\n",
    "\n",
    "# calculate value to normalize each vector by\n",
    "norm_factor = np.linalg.norm(text_emb, axis=1)\n",
    "norm_factor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Model\n",
    "# Getting image embeddings from CLIP\n",
    "img_inputs = processor(\n",
    "    text=None,\n",
    "    images=images,\n",
    "    return_tensors='pt'\n",
    ")['pixel_values'].to(device)\n",
    "img_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Model\n",
    "img_emb = model.get_image_features(img_inputs)\n",
    "print(img_emb.shape)\n",
    "print(img_emb.min(), img_emb.max())\n",
    "\n",
    "# NORMALIZE\n",
    "# detach text emb from graph, move to CPU, and convert to numpy array\n",
    "img_emb = img_emb.detach().cpu().numpy()\n",
    "\n",
    "img_emb = img_emb.T / np.linalg.norm(img_emb, axis=1)\n",
    "# transpose back to (21, 512)\n",
    "img_emb = img_emb.T\n",
    "print(img_emb.shape)\n",
    "print(img_emb.min(), img_emb.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Model\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cos_sim = np.dot(text_emb, img_emb.T) / (\n",
    "    norm(text_emb, axis=1) * norm(img_emb, axis=1)\n",
    ")\n",
    "cos_sim.shape\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(cos_sim)\n",
    "plt.title(\"Cosine Similarity Heatmap for CLIP Text-Image Embeddings\")\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Image Embeddings\")\n",
    "plt.ylabel(\"Text Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize both image and text embeddings\n",
    "image_embeddings = img_emb / np.linalg.norm(img_emb, axis=1, keepdims=True)\n",
    "text_embeddings = text_emb / np.linalg.norm(text_emb, axis=1, keepdims=True)\n",
    "\n",
    "# Compute cosine similarity between images and text (each row corresponds to one image-text pair)\n",
    "image_text_sim = np.dot(image_embeddings, text_embeddings.T)\n",
    "\n",
    "# Visualize the similarity matrix\n",
    "plt.imshow(image_text_sim, cmap=\"viridis\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = np.dot(image_embeddings, text_embeddings.T).flatten()\n",
    "\n",
    "plt.hist(similarities, bins=100, color=\"blue\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Image-Text Similarities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently holding embeddings from DINOv2 + sample information in payloads\n",
    "# Loading Qdrant database access tokens from .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Qdrant client object\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qclient = QdrantClient(\n",
    "    url= os.getenv(\"QDRANT_DB_URL\"),\n",
    "    api_key= os.getenv(\"QDRANT_API_KEY\")\n",
    ")\n",
    "qclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating collection in Qdrant database \n",
    "\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "collection_name = \"sample_images_2\"\n",
    "collection = qclient.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=embedding_len,\n",
    "\n",
    "        # Previously tried DOT distance, but cosine distance is more suitable for image embeddings\n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONifying the payloads dataframe to format metadata for each point\n",
    "\n",
    "payload_dicts = payloads.to_dict(orient=\"records\")\n",
    "payload_dicts[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating records of payloads to load into Qdrant\n",
    "\n",
    "from qdrant_client import models\n",
    "\n",
    "records = [\n",
    "    models.Record(\n",
    "        id=idx,\n",
    "        payload=payload_dicts[idx],\n",
    "        vector=avg_patch_embeddings[idx]\n",
    "    )\n",
    "    for idx, _ in enumerate(payload_dicts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending records to Qdrant database\n",
    "\n",
    "qclient.upload_records(\n",
    "    collection_name=collection_name,\n",
    "    records=records\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qdrant_client import QdrantClient\n",
    "# from qdrant_client.models import Distance, VectorParams\n",
    "# from qdrant_client.models import PointStruct\n",
    "\n",
    "\n",
    "\n",
    "# client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# # client.create_collection(\n",
    "# #     collection_name=\"test_collection\",\n",
    "# #     vectors_config=VectorParams(size=4, distance=Distance.DOT),\n",
    "# # )\n",
    "\n",
    "# # operation_info = client.upsert(\n",
    "# #     collection_name=\"test_collection\",\n",
    "# #     wait=True,\n",
    "# #     points=[\n",
    "# #         PointStruct(id=1, vector=[0.05, 0.61, 0.76, 0.74], payload={\"city\": \"Berlin\"}),\n",
    "# #         PointStruct(id=2, vector=[0.19, 0.81, 0.75, 0.11], payload={\"city\": \"London\"}),\n",
    "# #         PointStruct(id=3, vector=[0.36, 0.55, 0.47, 0.94], payload={\"city\": \"Moscow\"}),\n",
    "# #         PointStruct(id=4, vector=[0.18, 0.01, 0.85, 0.80], payload={\"city\": \"New York\"}),\n",
    "# #         PointStruct(id=5, vector=[0.24, 0.18, 0.22, 0.44], payload={\"city\": \"Beijing\"}),\n",
    "# #         PointStruct(id=6, vector=[0.35, 0.08, 0.11, 0.44], payload={\"city\": \"Mumbai\"}),\n",
    "# #     ],\n",
    "# # )\n",
    "\n",
    "# # print(operation_info)\n",
    "\n",
    "# # search_result = client.query_points(\n",
    "# #     collection_name=\"test_collection\",\n",
    "# #     query=[0.2, 0.1, 0.9, 0.7],\n",
    "# #     with_payload=False,\n",
    "# #     limit=3\n",
    "# # ).points\n",
    "\n",
    "# # print(search_result)\n",
    "\n",
    "# client.delete_collection(collection_name=\"test_collection\")\n",
    "# print(f\"Collection 'test_collection' deleted.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
