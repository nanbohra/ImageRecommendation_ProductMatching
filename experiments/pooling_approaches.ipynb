{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7594d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out different methods of pooling patch embeddings from DINOv2 hidden states\n",
    "# Baselines were just average pooling and using the CLS token\n",
    "# I try max pooling and combined max / avg pooling embeddings here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f69acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nandinibohra/Desktop/VSCodeFiles/ImageRecommendation_ProductMatching/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbb36a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dinov2Model(\n",
       "  (embeddings): Dinov2Embeddings(\n",
       "    (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): Dinov2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x Dinov2Layer(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attention): Dinov2Attention(\n",
       "          (attention): Dinov2SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): Dinov2SelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layer_scale1): Dinov2LayerScale()\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Dinov2MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_scale2): Dinov2LayerScale()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceceb325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach #1: Average pooling (this was what was done in the baseline)\n",
    "\n",
    "def get_avg_emb(path):\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    patches = outputs.last_hidden_state[:, 1:, :]\n",
    "    # print(f\"Patches shape: {patches.shape}\")\n",
    "    emb = torch.mean(patches, dim=1)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    # print(f\"Embedding shape (before squeeze): {emb.shape}\")\n",
    "    return emb.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c12deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach #2: Max pooling; take max val from each patch\n",
    "\n",
    "def get_max_emb(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    patches = outputs.last_hidden_state[:, 1:, :]\n",
    "    emb = torch.max(patches, dim=1)[0] # Max pooling returns (max values, indices), onyl keep values\n",
    "    emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Squeeze down to make a vec\n",
    "    return emb.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b519cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach #3: Combined avg and max pooling\n",
    "# Concatenate avg pool vectors and max pooled vectors\n",
    "\n",
    "def get_comb_emb(path):\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    inputs = processor(image, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    patches = outputs.last_hidden_state[:, 1:, :]\n",
    "\n",
    "    avg = torch.mean(patches, dim=1)\n",
    "    maxed = torch.max(patches, dim=1)[0] \n",
    "\n",
    "    combined = torch.cat([avg, maxed], dim=1)\n",
    "    combined = combined / combined.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return combined.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "242e678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach #4: CLS token only (another baseline measure)\n",
    "\n",
    "def get_cls_emb(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    cls = outputs.last_hidden_state[:, 0, :]\n",
    "    cls = cls / cls.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Squeeze down to make a vec\n",
    "    return cls.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14965fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import in dataframe of image payloads\n",
    "payloads = pd.read_csv(\"payloads.csv\")\n",
    "# payloads.head()\n",
    "\n",
    "image_paths = payloads['image_url'] \n",
    "materials = payloads['material']\n",
    "color_labels = payloads[\"color label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2ce9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_tuple(s):\n",
    "    # extract all integers from the string\n",
    "    if isinstance(s, str):\n",
    "        s = s.replace(\"int64\",\"\")\n",
    "        nums = list(map(int, re.findall(r\"\\d+\", s)))\n",
    "        return tuple(nums)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "037be9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     (230, 223, 199)\n",
       "1     (223, 186, 158)\n",
       "2     (162, 191, 156)\n",
       "3     (241, 233, 218)\n",
       "4     (144, 123, 100)\n",
       "5        (94, 69, 46)\n",
       "6        (23, 92, 87)\n",
       "7     (176, 154, 126)\n",
       "8        (92, 90, 78)\n",
       "9     (188, 183, 171)\n",
       "10    (121, 111, 102)\n",
       "11    (200, 204, 203)\n",
       "12    (223, 196, 185)\n",
       "13    (167, 153, 124)\n",
       "14    (154, 155, 146)\n",
       "15    (167, 165, 158)\n",
       "16    (199, 197, 189)\n",
       "17       (94, 69, 46)\n",
       "18    (234, 235, 222)\n",
       "19       (74, 70, 63)\n",
       "20    (233, 214, 181)\n",
       "21    (240, 224, 202)\n",
       "22    (233, 217, 185)\n",
       "23    (224, 227, 201)\n",
       "24    (229, 230, 203)\n",
       "25    (237, 231, 200)\n",
       "26    (234, 221, 187)\n",
       "27    (242, 228, 192)\n",
       "28    (194, 174, 134)\n",
       "29    (153, 161, 138)\n",
       "30     (90, 114, 101)\n",
       "31    (174, 191, 176)\n",
       "32    (204, 208, 189)\n",
       "33    (202, 208, 187)\n",
       "34    (172, 197, 169)\n",
       "35    (230, 227, 202)\n",
       "36    (236, 235, 211)\n",
       "37    (201, 201, 188)\n",
       "38    (229, 197, 168)\n",
       "39    (236, 205, 176)\n",
       "40    (189, 163, 127)\n",
       "41    (225, 220, 186)\n",
       "42    (227, 233, 207)\n",
       "43    (202, 218, 191)\n",
       "44    (229, 225, 213)\n",
       "45    (234, 227, 215)\n",
       "46    (215, 188, 138)\n",
       "47    (138, 132, 114)\n",
       "48       (98, 95, 83)\n",
       "49    (137, 132, 121)\n",
       "Name: avg rgb, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payloads[\"avg rgb\"] = payloads[\"avg rgb\"].apply(clean_tuple)\n",
    "avg_rgb = payloads[\"avg rgb\"]\n",
    "avg_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d456bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate l2 distance btw two RGB colors\n",
    "# 0 if exact match\n",
    "# ~441 if exact opposite\n",
    "def color_distance(rgb1, rgb2):\n",
    "    return np.sqrt(sum((int(a) - int(b))**2 for a, b in zip(rgb1, rgb2)))\n",
    "\n",
    "# Check threshold\n",
    "def are_colors_similar(rgb1, rgb2, threshold=50):\n",
    "    return color_distance(rgb1, rgb2) < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c78fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing: AVG\n",
      "==================================================\n",
      "Embedding shape: (50, 768)\n",
      "  Mean similarity: 0.549\n",
      "  Std similarity:  0.222\n",
      "  High-sim pairs (≥0.6): 440\n",
      "  Material match rate: 33.6%\n",
      "  Color label match: 6.1%\n",
      "  Color RGB similar: 68.4%\n",
      "\n",
      "==================================================\n",
      "Testing: MAX\n",
      "==================================================\n",
      "Embedding shape: (50, 768)\n",
      "  Mean similarity: 0.913\n",
      "  Std similarity:  0.035\n",
      "  High-sim pairs (≥0.6): 1225\n",
      "  Material match rate: 12.1%\n",
      "  Color label match: 4.7%\n",
      "  Color RGB similar: 59.4%\n",
      "\n",
      "==================================================\n",
      "Testing: COMBINED\n",
      "==================================================\n",
      "Embedding shape: (50, 1536)\n",
      "  Mean similarity: 0.857\n",
      "  Std similarity:  0.063\n",
      "  High-sim pairs (≥0.6): 1225\n",
      "  Material match rate: 12.1%\n",
      "  Color label match: 4.7%\n",
      "  Color RGB similar: 59.4%\n",
      "\n",
      "==================================================\n",
      "Testing: CLS\n",
      "==================================================\n",
      "Embedding shape: (50, 768)\n",
      "  Mean similarity: 0.418\n",
      "  Std similarity:  0.279\n",
      "  High-sim pairs (≥0.6): 412\n",
      "  Material match rate: 35.9%\n",
      "  Color label match: 6.3%\n",
      "  Color RGB similar: 70.1%\n"
     ]
    }
   ],
   "source": [
    "methods = {\n",
    "    'avg': get_avg_emb,\n",
    "    'max': get_max_emb,\n",
    "    'combined': get_comb_emb,\n",
    "    'cls': get_cls_emb\n",
    "}\n",
    "\n",
    "for method_name, method_func in methods.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing: {method_name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = []\n",
    "    for path in image_paths:\n",
    "        emb = method_func(path)\n",
    "        embeddings.append(emb)\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    print(f\"Embedding shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Get statistics\n",
    "    upper_tri = np.triu_indices_from(sim_matrix, k=1)\n",
    "    similarities = sim_matrix[upper_tri]\n",
    "\n",
    "    # Analyze pairs with high similarity\n",
    "    high_sim_pairs = []\n",
    "    for i in range(len(sim_matrix)):\n",
    "        for j in range(i+1, len(sim_matrix)):\n",
    "            if sim_matrix[i, j] >= 0.6:\n",
    "                high_sim_pairs.append({\n",
    "                    'mat_match': materials[i] == materials[j],\n",
    "                    'col_match': color_labels[i] == color_labels[j],\n",
    "                    'col_rgb_distance': color_distance(avg_rgb[i], avg_rgb[j]),\n",
    "                    'col_rgb_similarity': color_distance(avg_rgb[i], avg_rgb[j]) < 100\n",
    "                })\n",
    "    \n",
    "    mat_match_rate = np.mean([p['mat_match'] for p in high_sim_pairs]) if high_sim_pairs else 0\n",
    "    col_label_match_rate = np.mean([p['col_match'] for p in high_sim_pairs]) if high_sim_pairs else 0\n",
    "    col_rgb_similar_rate = np.mean([p['col_rgb_similarity'] for p in high_sim_pairs]) if high_sim_pairs else 0\n",
    "    \n",
    "    \n",
    "    print(f\"  Mean similarity: {similarities.mean():.3f}\")\n",
    "    print(f\"  Std similarity:  {similarities.std():.3f}\")\n",
    "    print(f\"  High-sim pairs (≥0.6): {len(high_sim_pairs)}\")\n",
    "    print(f\"  Material match rate: {mat_match_rate:.1%}\")\n",
    "    print(f\"  Color label match: {col_label_match_rate:.1%}\")\n",
    "    print(f\"  Color RGB similar: {col_rgb_similar_rate:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best so far is CLS token\n",
    "# High sim pairs are showing 70% match in average color\n",
    "# and 35% match for material\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
