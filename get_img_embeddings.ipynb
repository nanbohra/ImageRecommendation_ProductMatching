{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nScript to generate embeddings for images (for input and catalog images) and store catalog embeddings to Qdrant database.\\n@File    : embeddings.py\\n@Date    : 2025-03-04\\n@Author  : Nandini Bohra\\n@Contact : nbohra@ucsd.edu\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to generate embeddings for images (for input and catalog images) and store catalog embeddings to Qdrant database.\n",
    "@File    : embeddings.py\n",
    "@Date    : 2025-03-04\n",
    "@Author  : Nandini Bohra\n",
    "@Contact : nbohra@ucsd.edu\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_48.jpg',\n",
       " 'sample_49.jpg',\n",
       " 'sample_11.jpg',\n",
       " 'sample_39.jpg',\n",
       " 'sample_38.jpg',\n",
       " 'sample_10.jpg',\n",
       " 'sample_12.jpg',\n",
       " 'sample_13.jpg',\n",
       " 'sample_17.jpg',\n",
       " 'sample_16.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_directory = \"/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images\"\n",
    "all_img_files = os.listdir(base_directory)\n",
    "all_img_files[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_48.jpg',\n",
       " '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_49.jpg',\n",
       " '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_11.jpg',\n",
       " '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_39.jpg',\n",
       " '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_38.jpg',\n",
       " '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_10.jpg',\n",
       " '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_12.jpg',\n",
       " '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_13.jpg',\n",
       " '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_17.jpg',\n",
       " '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_16.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_urls = list(map(lambda x: os.path.join(base_directory, x), all_img_files))\n",
    "all_img_urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/nandinibohra/Desktop/VSCodeFiles/Arthta...</td>\n",
       "      <td>samples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/nandinibohra/Desktop/VSCodeFiles/Arthta...</td>\n",
       "      <td>samples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/nandinibohra/Desktop/VSCodeFiles/Arthta...</td>\n",
       "      <td>samples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/nandinibohra/Desktop/VSCodeFiles/Arthta...</td>\n",
       "      <td>samples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/nandinibohra/Desktop/VSCodeFiles/Arthta...</td>\n",
       "      <td>samples</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_url     type\n",
       "0  /Users/nandinibohra/Desktop/VSCodeFiles/Arthta...  samples\n",
       "1  /Users/nandinibohra/Desktop/VSCodeFiles/Arthta...  samples\n",
       "2  /Users/nandinibohra/Desktop/VSCodeFiles/Arthta...  samples\n",
       "3  /Users/nandinibohra/Desktop/VSCodeFiles/Arthta...  samples\n",
       "4  /Users/nandinibohra/Desktop/VSCodeFiles/Arthta...  samples"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from PIL import Image\n",
    "\n",
    "payloads = DataFrame.from_records({\"image_url\": all_img_urls})\n",
    "payloads[\"type\"] = \"samples\"\n",
    "payloads.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1700x920>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1700x920>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1700x920>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1700x920>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1700x920>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1700x920>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1700x920>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1700x920>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1700x920>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1700x920>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = list(map(lambda x: Image.open(x), payloads[\"image_url\"]))\n",
    "images[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize images and conver to base 64 rep if needed to show on front end\n",
    "\n",
    "from io import BytesIO\n",
    "import math\n",
    "import base64\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-13.0327, -12.8485, -11.5226,  ..., -13.6024, -11.4868,  -9.8092],\n",
       "        [ -8.8716, -10.5682, -10.8089,  ..., -12.5314,  -8.5624,  -8.4759],\n",
       "        [ -8.2020,  -8.5568,  -8.1728,  ..., -10.2424,  -7.6316,  -6.6405],\n",
       "        ...,\n",
       "        [-12.1440, -12.1543, -10.0716,  ..., -11.9698,  -7.9782,  -8.2962],\n",
       "        [-12.3966, -11.1451, -12.2452,  ..., -12.0119,  -9.8162, -10.8850],\n",
       "        [ -9.0621,  -7.6530,  -6.6977,  ...,  -9.7274,  -7.3580,  -6.0283]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trial with Microsoft Resnet-50 model\n",
    "# https://huggingface.co/microsoft/resnet-50\n",
    "\n",
    "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
    "model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
    "\n",
    "inputs = processor(\n",
    "    images, \n",
    "    return_tensors=\"pt\", \n",
    "    # padding=True\n",
    ")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "embeddings = outputs.logits\n",
    "embeddings\n",
    "\n",
    "# Evaluated embeddings... not sure if this is the right fit \n",
    "# Researching and trying other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 256, 768])\n",
      "torch.Size([50, 768])\n"
     ]
    }
   ],
   "source": [
    "# Trying with DINO V2 model\n",
    "# https://huggingface.co/facebook/dinov2-base\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import torch\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "inputs = processor(\n",
    "    images, \n",
    "    return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "hidden_states = outputs.last_hidden_state # Shape: [batch_size=50, num_tokens=1+256, hidden_size=768]\n",
    "\n",
    "# Removing CLS Token and taking average of all patch embeddings\n",
    "all_patch_embedding = hidden_states[:, 1:, :]\n",
    "# print(all_patch_embedding.shape)\n",
    "\n",
    "avg_patch_embeddings = torch.mean(all_patch_embedding, dim=1)\n",
    "# print(avg_patch_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_len = len(avg_patch_embeddings[0])\n",
    "embedding_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<qdrant_client.qdrant_client.QdrantClient at 0x36c426000>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qclient = QdrantClient(\n",
    "    url= os.getenv(\"QDRANT_DB_URL\"),\n",
    "    api_key= os.getenv(\"QDRANT_API_KEY\")\n",
    ")\n",
    "qclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/h22vp3dd135f5npjdy_vlh1c0000gn/T/ipykernel_4608/2262020600.py:4: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  collection = qclient.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "collection_name = \"sample_images_2\"\n",
    "collection = qclient.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=embedding_len,\n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_url': '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_48.jpg',\n",
       "  'type': 'samples'},\n",
       " {'image_url': '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_49.jpg',\n",
       "  'type': 'samples'},\n",
       " {'image_url': '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_11.jpg',\n",
       "  'type': 'samples'},\n",
       " {'image_url': '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_39.jpg',\n",
       "  'type': 'samples'},\n",
       " {'image_url': '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_38.jpg',\n",
       "  'type': 'samples'},\n",
       " {'image_url': '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_10.jpg',\n",
       "  'type': 'samples'},\n",
       " {'image_url': '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_12.jpg',\n",
       "  'type': 'samples'},\n",
       " {'image_url': '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_13.jpg',\n",
       "  'type': 'samples'},\n",
       " {'image_url': '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_17.jpg',\n",
       "  'type': 'samples'},\n",
       " {'image_url': '/Users/nandinibohra/Desktop/VSCodeFiles/Arthtattva_InternAssignment_Mar2025/ImageReco-ProductMatching/Product_Catalog/all_product_images/sample_images/sample_16.jpg',\n",
       "  'type': 'samples'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload_dicts = payloads.to_dict(orient=\"records\")\n",
    "payload_dicts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "records = [\n",
    "    models.Record(\n",
    "        id=idx,\n",
    "        payload=payload_dicts[idx],\n",
    "        vector=avg_patch_embeddings[idx]\n",
    "    )\n",
    "    for idx, _ in enumerate(payload_dicts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/h22vp3dd135f5npjdy_vlh1c0000gn/T/ipykernel_4608/3861376132.py:1: DeprecationWarning: `upload_records` is deprecated, use `upload_points` instead\n",
      "  qclient.upload_records(\n"
     ]
    }
   ],
   "source": [
    "qclient.upload_records(\n",
    "    collection_name=collection_name,\n",
    "    records=records\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qdrant_client import QdrantClient\n",
    "# from qdrant_client.models import Distance, VectorParams\n",
    "# from qdrant_client.models import PointStruct\n",
    "\n",
    "\n",
    "\n",
    "# client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# # client.create_collection(\n",
    "# #     collection_name=\"test_collection\",\n",
    "# #     vectors_config=VectorParams(size=4, distance=Distance.DOT),\n",
    "# # )\n",
    "\n",
    "# # operation_info = client.upsert(\n",
    "# #     collection_name=\"test_collection\",\n",
    "# #     wait=True,\n",
    "# #     points=[\n",
    "# #         PointStruct(id=1, vector=[0.05, 0.61, 0.76, 0.74], payload={\"city\": \"Berlin\"}),\n",
    "# #         PointStruct(id=2, vector=[0.19, 0.81, 0.75, 0.11], payload={\"city\": \"London\"}),\n",
    "# #         PointStruct(id=3, vector=[0.36, 0.55, 0.47, 0.94], payload={\"city\": \"Moscow\"}),\n",
    "# #         PointStruct(id=4, vector=[0.18, 0.01, 0.85, 0.80], payload={\"city\": \"New York\"}),\n",
    "# #         PointStruct(id=5, vector=[0.24, 0.18, 0.22, 0.44], payload={\"city\": \"Beijing\"}),\n",
    "# #         PointStruct(id=6, vector=[0.35, 0.08, 0.11, 0.44], payload={\"city\": \"Mumbai\"}),\n",
    "# #     ],\n",
    "# # )\n",
    "\n",
    "# # print(operation_info)\n",
    "\n",
    "# # search_result = client.query_points(\n",
    "# #     collection_name=\"test_collection\",\n",
    "# #     query=[0.2, 0.1, 0.9, 0.7],\n",
    "# #     with_payload=False,\n",
    "# #     limit=3\n",
    "# # ).points\n",
    "\n",
    "# # print(search_result)\n",
    "\n",
    "# client.delete_collection(collection_name=\"test_collection\")\n",
    "# print(f\"Collection 'test_collection' deleted.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
